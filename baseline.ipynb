{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f975bda-0fc1-400f-abd8-34d41572d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Model/Training related libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Logging\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Dataloader libraries\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "# Custom models/loss/dataloaders/utilities\n",
    "from models import SiameseNet_OntologicalLayer, YamnetSiameseNet_OntologicalLayer\n",
    "from loss import Ontological_Loss, Ontological_Loss_Unweighted\n",
    "from dataloaders import AudioSet_Siamese, AudioSet_Siamese_Eval, AudioSet_Strong_Siamese, AudioSet_Strong_Siamese_Eval\n",
    "from sklearn import metrics\n",
    "import scipy.io\n",
    "\n",
    "# GPU\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3b0ccc-5e60-4362-a3a8-c847632f062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23  0 16 14]\n",
      "[ 0 16]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = './data/'\n",
    "sounds_data = np.load(data_dir + 'audioset_strong_train1_data.npy', allow_pickle=True)\n",
    "class1_index = np.load(data_dir + 'audioset_strong_train1_labels_1.npy', allow_pickle=True)\n",
    "class2_index = np.load(data_dir + 'audioset_strong_train1_labels_2.npy', allow_pickle=True)\n",
    "# Dataloader\n",
    "train_data = AudioSet_Strong_Siamese(sounds_data, class1_index, class2_index, 37, 7)\n",
    "train_args = dict(shuffle = True, batch_size = 256, num_workers=8, pin_memory=True)\n",
    "train_loader = DataLoader(train_data, **train_args)\n",
    "\n",
    "sounds_data = np.load(data_dir + 'audioset_strong_val_data.npy', allow_pickle=True)\n",
    "class1_index = np.load(data_dir + 'audioset_strong_val_labels_1.npy', allow_pickle=True)\n",
    "class2_index = np.load(data_dir + 'audioset_strong_val_labels_2.npy', allow_pickle=True)\n",
    "\n",
    "# val_idx = np.random.choice(len(sounds_data), int(0.2*train_data.data.shape[0]/10))\n",
    "# sounds_data = sounds_data[val_idx]\n",
    "# class2_index = class2_index[val_idx]\n",
    "# val_idx_extended = np.zeros((len(val_idx), 10))\n",
    "# for i in range(len(val_idx)):\n",
    "#     val_idx_extended[i] = np.arange(val_idx[i], val_idx[i] + 10)\n",
    "# val_idx_extended = val_idx_extended.reshape(-1, 1).astype('int')\n",
    "# class1_index = class1_index[val_idx_extended]    \n",
    "\n",
    "val_data = AudioSet_Strong_Siamese(sounds_data, class1_index, class2_index, 37, 7)\n",
    "val_args = dict(shuffle = False, batch_size = 256, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, **val_args)\n",
    "\n",
    "eval_data = AudioSet_Strong_Siamese_Eval(sounds_data, class1_index, class2_index, 37, 7)\n",
    "eval_args = dict(shuffle = False, batch_size = 256, num_workers=8, pin_memory=True, sampler=SequentialSampler(eval_data))\n",
    "eval_loader = DataLoader(eval_data, **eval_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10eebb-d2f8-459b-b120-593cddbb1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sounds_data))\n",
    "print(len(class1_index))\n",
    "\n",
    "class1_index_cat = np.concatenate(class1_index, axis=0)\n",
    "class1_index_cat = [np.asarray(lbl) for lbl in class1_index_cat]\n",
    "# print(class1_index_cat)\n",
    "print(class1_index_cat[10].dtype)\n",
    "\n",
    "print(train_data.labels1[:40])\n",
    "print(val_data.labels1[:40])\n",
    "print(train_data.labels2[:40])\n",
    "print(val_data.labels2[:40])\n",
    "\n",
    "print(train_data.logits_1[:10])\n",
    "# print(eval_data.labels1)\n",
    "\n",
    "print(len(train_data.logits_1))\n",
    "print(len(train_data.logits_2))\n",
    "print(len(train_data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994d49d-c897-45cd-9591-d52482316ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_dir = './test/audioset_strong/'\n",
    "sounds_data = np.load(data_dir + 'audioset_strong_bal_train_data.npy', allow_pickle=True)\n",
    "class1_index = np.load(data_dir + 'audioset_strong_bal_train_labels_1.npy', allow_pickle=True)\n",
    "# class2_index = np.load(data_dir + 'audioset_strong_bal_train_labels_2.npy', allow_pickle=True)\n",
    "\n",
    "data = np.concatenate(sounds_data, axis=0).astype('float')\n",
    "seg_per_clip = 10\n",
    "length = data.shape[0]\n",
    "labels1 = np.concatenate(class1_index, axis=0)\n",
    "labels1 = [np.asarray(lbl) for lbl in labels1]\n",
    "# labels2 = np.concatenate(class2_index, axis=0)        \n",
    "# labels2 = [np.asarray(lbl) for lbl in labels2]\n",
    "\n",
    "num_clips = data.shape[0]\n",
    "num_subclasses = 42\n",
    "# num_superclasses = 7\n",
    "\n",
    "logits_1 = np.zeros((num_clips, num_subclasses)).astype('long')\n",
    "# logits_2 = np.zeros((num_clips, num_superclasses)).astype('long')\n",
    "\n",
    "for i in range(num_clips):\n",
    "    logits_1[i][labels1[i].astype('int')] = 1 \n",
    "    # logits_2[i][labels2[i].astype('int')] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad4692-3e9d-4a61-a94e-25b9b7bf3381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(np.sum(logits_1, axis=0) != 0))\n",
    "# print(np.sum(logits_2, axis=0))\n",
    "\n",
    "class_to_condensed = np.zeros((42,))\n",
    "idx = np.where(np.sum(logits_1, axis=0) != 0)[0]\n",
    "for i in range(len(idx)):\n",
    "    class_to_condensed[idx[i]] = i\n",
    "    \n",
    "print(class_to_condensed)\n",
    "\n",
    "condensed_labels = []\n",
    "for lbls in labels1:\n",
    "    condensed_labels.append(class_to_condensed[lbls].astype('int'))\n",
    "# np.save('./data/audioset_strong_eval_labels_1.npy', np.asanyarray(condensed_labels, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3a2cf-3986-46ee-b357-4ea58e73e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './test/audioset_strong/'\n",
    "# sounds_data = np.load(data_dir + 'audioset_strong_bal_train_data.npy', allow_pickle=True)\n",
    "class1_index = np.load(data_dir + 'audioset_strong_eval_labels_1.npy', allow_pickle=True)\n",
    "labels1 = np.concatenate(class1_index, axis=0)\n",
    "labels1 = [np.asarray(lbl) for lbl in labels1]\n",
    "condensed_labels = []\n",
    "for lbls in labels1:\n",
    "    condensed_labels.append(class_to_condensed[lbls].astype('int'))\n",
    "np.save('./data/audioset_strong_eval_labels_1.npy', np.asanyarray(condensed_labels, dtype=object))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9ff60-93e2-4747-955e-b0aec0d4b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.logits_1.shape)\n",
    "print(np.sum(train_data.logits_1, axis=0))\n",
    "print(np.sum(eval_data.logits_1, axis=0))\n",
    "print(np.sum(val_data.logits_1, axis=0))\n",
    "\n",
    "print(eval_data.logits_1.shape)\n",
    "print(val_data.logits_1.shape)\n",
    "\n",
    "print(len(sounds_data))\n",
    "print(len(class1_index))\n",
    "print(len(class2_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f8273b-0e62-4cb2-bcc1-98b4f0733b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontology Layer \n",
    "M = np.asarray([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
    "                [0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
    "                [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "# for strongly labeled data\n",
    "condensed_idx = [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
    "       17, 18, 19, 20, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38,\n",
    "       39, 40, 41]\n",
    "M = M[:, condensed_idx]\n",
    "M = M / np.sum(M, axis=1).reshape(-1, 1)\n",
    "\n",
    "# Siamese Net Model\n",
    "in_feature_dim = train_data.__getitem__(0)[0].shape[0]\n",
    "model = SiameseNet_OntologicalLayer(7, 37, in_feature_dim, M)\n",
    "model.to(device)\n",
    "\n",
    "# Define Loss function\n",
    "lambda1 = 1.75\n",
    "lambda2 = 1\n",
    "lambda3 = 0.25\n",
    "\n",
    "# Weights for BCE Loss \n",
    "tot1 = np.sum(train_data.logits_1)\n",
    "pos_weights_1 = np.sum(train_data.logits_1, axis=0)\n",
    "neg_weights_1 = train_data.logits_1.shape[0] - pos_weights_1\n",
    "tot2 = np.sum(train_data.logits_2)\n",
    "pos_weights_2 = np.sum(train_data.logits_2, axis=0)\n",
    "neg_weights_2 = train_data.logits_2.shape[0] - pos_weights_2\n",
    "label_weights_1 = torch.tensor(neg_weights_1/pos_weights_1).to(device)\n",
    "label_weights_2 = torch.tensor(neg_weights_2/pos_weights_2).to(device)\n",
    "\n",
    "criterion = Ontological_Loss(lambda1, lambda2, lambda3, label_weights_1, label_weights_2)\n",
    "\n",
    "# Define Optimizer\n",
    "learningRate = 2e-4\n",
    "weightDecay = 1e-2\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=weightDecay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6daebc0a-ac3e-4786-a23e-f616741918bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train ###\n",
    "def train_model(train_loader, model, epoch, writer):\n",
    "    \n",
    "    training_loss = 0\n",
    "    training_acc_1 = 0\n",
    "    training_acc_2 = 0\n",
    "    \n",
    "    # Set model in 'Training mode'\n",
    "    model.train()\n",
    "    \n",
    "    # enumerate mini batches\n",
    "    with tqdm(train_loader, ) as t_epoch:\n",
    "        for i, (input1, input2, target1_1, target1_2, target2_1, target2_2, pair_type) in enumerate(t_epoch):\n",
    "            t_epoch.set_description(f\"Epoch {epoch}\")\n",
    "            \n",
    "            # Move to GPU\n",
    "            input1 = input1.to(device).float()\n",
    "            target1_1 = target1_1.to(device)\n",
    "            target1_2 = target1_2.to(device)\n",
    "            \n",
    "            # print(target1_1.size())\n",
    "            \n",
    "            input2 = input2.to(device).float()\n",
    "            target2_1 = target2_1.to(device)\n",
    "            target2_2 = target2_2.to(device)\n",
    "            \n",
    "            # print(target1_1)\n",
    "            # print(target1_2)\n",
    "            # print(pair_type)\n",
    "            \n",
    "            # print(pair_type)\n",
    "            pair_type = pair_type.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Model output\n",
    "            outputs = model.forward(input1, input2) \n",
    "            targets = (target1_1, target1_2, target2_1, target2_2)\n",
    "            \n",
    "            # Loss/Backprop\n",
    "            loss = criterion(outputs, targets, pair_type)                           \n",
    "            loss.backward()\n",
    "            optimizer.step()     \n",
    "\n",
    "            training_loss += loss.item()\n",
    "            t_epoch.set_postfix(loss=loss.item())\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del input1, input2\n",
    "            del target1_1, target1_2, target2_1, target2_2, pair_type\n",
    "            del loss\n",
    "    \n",
    "    training_loss /= len(train_loader)\n",
    "    writer.add_scalar(\"Loss/train\", training_loss, epoch)  \n",
    "    \n",
    "    return training_loss\n",
    "\n",
    "\n",
    "# Validation\n",
    "def evaluate_model(val_loader, model, epoch, writer):\n",
    "        \n",
    "    val_loss = 0\n",
    "\n",
    "    # Set model in validation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for i, (input1, input2, target1_1, target1_2, target2_1, target2_2, pair_type) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Move to GPU\n",
    "            input1 = input1.to(device).float()\n",
    "            target1_1 = target1_1.to(device)\n",
    "            target1_2 = target1_2.to(device)\n",
    "            \n",
    "            input2 = input2.to(device).float()\n",
    "            target2_1 = target2_1.to(device)\n",
    "            target2_2 = target2_2.to(device)\n",
    "            \n",
    "            pair_type = pair_type.to(device)\n",
    "            \n",
    "            # Model Output\n",
    "            outputs = model.forward(input1, input2) # model output\n",
    "            targets = (target1_1, target1_2, target2_1, target2_2)\n",
    "            \n",
    "            # Val loss\n",
    "            loss = criterion(outputs, targets, pair_type)            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    writer.add_scalar(\"Loss/val\", val_loss / len(val_loader), epoch)  \n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b57013f4-8ff5-4e56-acda-681ccb4757c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_stats(data_loader, model, reduction='weighted'):\n",
    "    \n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    complete_outputs_1 = []\n",
    "    complete_targets_1 = []\n",
    "    \n",
    "    complete_outputs_2 = []\n",
    "    complete_targets_2 = []\n",
    "    \n",
    "    # Evaluate test set in batches\n",
    "    for i, (input1, target1_1, target1_2) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Move to GPU\n",
    "            input1 = input1.to(device).float()\n",
    "            target1_1 = target1_1.to(device)\n",
    "            target1_2 = target1_2.to(device)\n",
    "            \n",
    "            batch_size = int(input1.shape[0]/2)\n",
    "            \n",
    "            # Model Output\n",
    "            _, _, out1_1, out1_2, out2_1, out2_2 = model.forward(input1[0:batch_size], input1[batch_size::]) # model output\n",
    "            \n",
    "            sigmoid = torch.nn.Sigmoid()\n",
    "            out1_1 = sigmoid(out1_1)\n",
    "            out2_1 = sigmoid(out2_1)\n",
    "\n",
    "            complete_outputs_1.append(torch.cat((out1_1, out2_1)))\n",
    "            complete_targets_1.append(target1_1)\n",
    "            \n",
    "            complete_outputs_2.append(torch.cat((out1_2, out2_2)))\n",
    "            complete_targets_2.append(target1_2)\n",
    "    \n",
    "    \n",
    "    # Concat batch results \n",
    "    complete_outputs_1 = torch.cat(complete_outputs_1, 0)\n",
    "    complete_targets_1 = torch.cat(complete_targets_1, 0)\n",
    "    \n",
    "    complete_outputs_2 = torch.cat(complete_outputs_2, 0)\n",
    "    complete_targets_2 = torch.cat(complete_targets_2, 0)\n",
    "    \n",
    "    # print(complete_outputs_1)\n",
    "    # print(complete_targets_1)\n",
    "    \n",
    "    num_classes_1 = complete_outputs_1.shape[-1]\n",
    "    num_classes_2 = complete_outputs_2.shape[-1]\n",
    "    \n",
    "    segs_per_clip = 10\n",
    "    \n",
    "    # Move to CPU\n",
    "    complete_targets_1 = complete_targets_1.detach().cpu().numpy()\n",
    "    complete_outputs_1 = complete_outputs_1.detach().cpu().numpy()\n",
    "    \n",
    "    complete_targets_2 = complete_targets_2.detach().cpu().numpy()\n",
    "    complete_outputs_2 = complete_outputs_2.detach().cpu().numpy()\n",
    "    \n",
    "    # Average outputs over entire audio clip\n",
    "    # output_1_avg = np.zeros((int(complete_outputs_1.shape[0]/segs_per_clip), complete_outputs_1.shape[1]))\n",
    "    # output_2_avg = np.zeros((int(complete_outputs_2.shape[0]/segs_per_clip), complete_outputs_2.shape[1]))\n",
    "    # for i in range(int(complete_outputs_1.shape[0]/segs_per_clip)):\n",
    "    #     output_1_avg[i] = np.mean(complete_outputs_1[segs_per_clip*i : segs_per_clip*(i+1)], axis=0)\n",
    "    #     output_2_avg[i] = np.mean(complete_outputs_2[segs_per_clip*i : segs_per_clip*(i+1)], axis=0)\n",
    "    \n",
    "    # print(output_1_avg)\n",
    "#     pos_weights_1 = np.sum(complete_targets_1[0::segs_per_clip], axis=0)\n",
    "#     pos_weights_2 = np.sum(complete_targets_2[0::segs_per_clip], axis=0)\n",
    "#     tot1 = np.sum(complete_targets_1[0::segs_per_clip])\n",
    "#     tot2 = np.sum(complete_targets_2[0::segs_per_clip])\n",
    "    \n",
    "#     weights_1 = pos_weights_1 / tot1\n",
    "#     weights_2 = pos_weights_2 / tot2\n",
    "\n",
    "    output_1_avg = complete_outputs_1\n",
    "    output_2_avg = complete_outputs_2\n",
    "\n",
    "    pos_weights_1 = np.sum(complete_targets_1, axis=0)\n",
    "    pos_weights_2 = np.sum(complete_targets_2, axis=0)\n",
    "    tot1 = np.sum(complete_targets_1)\n",
    "    tot2 = np.sum(complete_targets_2)\n",
    "    \n",
    "    weights_1 = pos_weights_1 / tot1\n",
    "    weights_2 = pos_weights_2 / tot2\n",
    "            \n",
    "    # Level 1 Average precision, AUC metrics\n",
    "    average_precision_1 = np.zeros((num_classes_1, ))\n",
    "    auc_1 = np.zeros((num_classes_1, ))\n",
    "    for i in range(num_classes_1):\n",
    "        average_precision_1[i] = metrics.average_precision_score(complete_targets_1[:, i], output_1_avg[:, i])\n",
    "        auc_1[i] = metrics.roc_auc_score(complete_targets_1[:, i], output_1_avg[:, i], average = None)\n",
    "\n",
    "    # Level 2 Average precision, AUC metrics\n",
    "    average_precision_2 = np.zeros((num_classes_2, ))\n",
    "    auc_2 = np.zeros((num_classes_2, ))\n",
    "    for i in range(num_classes_2):\n",
    "        average_precision_2[i] = metrics.average_precision_score(complete_targets_2[:, i], output_2_avg[:, i])\n",
    "        auc_2[i] = metrics.roc_auc_score(complete_targets_2[:, i], output_2_avg[:, i], average = None)\n",
    "    \n",
    "    result_dir = './results/'\n",
    "    scipy.io.savemat(result_dir + 'eval_metrics_test.mat', {'test_mAP_1': average_precision_1, 'test_mAP_2': average_precision_2, 'test_AUC_1': auc_1, 'test_AUC_2': auc_2})\n",
    "    \n",
    "    if(reduction=='average'):\n",
    "        mAP_1 = np.mean(average_precision_1)\n",
    "        mauc_1 = np.mean(auc_1)\n",
    "\n",
    "        mAP_2 = np.mean(average_precision_2)\n",
    "        mauc_2 = np.mean(auc_2)\n",
    "        \n",
    "    elif(reduction=='weighted'):\n",
    "        mAP_1 = np.sum(weights_1*average_precision_1)\n",
    "        mauc_1 = np.sum(weights_1*auc_1)\n",
    "        \n",
    "        mAP_2 = np.sum(weights_2*average_precision_2)\n",
    "        mauc_2 = np.sum(weights_2*auc_2)\n",
    "        \n",
    "    elif(reduction=='none'):\n",
    "        return average_precision_1, auc_1, complete_outputs_1, average_precision_2, auc_2, complete_outputs_2\n",
    "    \n",
    "    return mAP_1, mauc_1, complete_outputs_1, mAP_2, mauc_2, complete_outputs_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f142d60-735c-44af-8813-9b9b27bebfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_data.logits_1[0])\n",
    "print(np.sum(eval_data.logits_1, axis=0))\n",
    "print(np.sum(eval_data.logits_2, axis=0))\n",
    "print(eval_data.logits_1.shape)\n",
    "print(eval_data.logits_2.shape)\n",
    "print(eval_data.labels1[:50])\n",
    "print(train_data.labels1[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5aedff-88b5-4319-85c9-e63659e748b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c61602f7-8a9d-4b46-b298-8116846c64b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1952/1952 [06:12<00:00,  5.24it/s, loss=7.54]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (42,) (37,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_248096/2003508374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Val Stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mval_mAP_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplete_outputs_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mAP_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_auc_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplete_outputs_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_248096/3417452155.py\u001b[0m in \u001b[0;36mevaluate_model_stats\u001b[0;34m(data_loader, model, reduction)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mmAP_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0maverage_precision_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mmauc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mauc_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (42,) (37,) "
     ]
    }
   ],
   "source": [
    "# Save dirs\n",
    "model_num = 'siamese_strong_doubleweighted_lambda1_' + str(lambda1) + '_lambda2_' + str(lambda2) + '_lambda3_' + str(lambda3) + '/'\n",
    "base_dir = './'\n",
    "model_dir = base_dir + 'models/' + model_num\n",
    "runs_dir = base_dir + 'runs/' + model_num\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(runs_dir):\n",
    "    os.makedirs(runs_dir)\n",
    "\n",
    "# Tensorboard logging\n",
    "writer = SummaryWriter(runs_dir)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Training loop\n",
    "epochs = 75\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Train + Validation\n",
    "    training_loss = train_model(train_loader, model, epoch, writer)\n",
    "    val_loss = evaluate_model(val_loader, model, epoch, writer)\n",
    "    \n",
    "    # Val Stats\n",
    "    val_mAP_1, val_auc_1, complete_outputs_1, val_mAP_2, val_auc_2, complete_outputs_2 = evaluate_model_stats(eval_loader, model, reduction='weighted')\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print log of accuracy and loss\n",
    "    print(\"Epoch: \"+str(epoch)+\", Training loss: \"+str(training_loss)+\", Validation loss: \"+str(val_loss)+\", Validation mAP_1/AUC_1: \"+str(val_mAP_1)+\"/\"+str(val_auc_1)+\n",
    "             \", Validation mAP_2/AUC_2: \"+str(val_mAP_2)+\"/\"+str(val_auc_2))\n",
    "    \n",
    "    writer.add_scalar(\"mAP_1/val\", val_mAP_1, epoch)\n",
    "    writer.add_scalar(\"AUC_1/val\", val_auc_1, epoch)\n",
    "    writer.add_scalar(\"mAP_2/val\", val_mAP_2, epoch)\n",
    "    writer.add_scalar(\"AUC_2/val\", val_auc_2, epoch)\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    model_filename = model_dir + 'epoch' + str(epoch) + '.pt'\n",
    "    if(epoch == 0):\n",
    "        torch.save(model, model_filename)\n",
    "    else:\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': training_loss,}, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e623ebd-7f71-4cd5-b5e3-c19af0eaf77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dirs\n",
    "model_num = 'siamese_lambda1_' + str(lambda1) + '_lambda2_' + str(lambda2) + '_lambda3_' + str(lambda3) + '/'\n",
    "base_dir = './'\n",
    "model_dir = base_dir + 'models/' + model_num\n",
    "runs_dir = base_dir + 'runs/' + model_num\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(runs_dir):\n",
    "    os.makedirs(runs_dir)\n",
    "\n",
    "# Tensorboard logging\n",
    "writer = SummaryWriter(runs_dir)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Load model\n",
    "model_save_dir = model_dir + 'epoch0.pt'\n",
    "model = torch.load(model_save_dir) \n",
    "model.to(device)\n",
    "\n",
    "# Load saved weights\n",
    "weights_dir = model_dir + 'epoch43.pt'\n",
    "state = torch.load(weights_dir)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "# Training loop\n",
    "epochs = 40\n",
    "for epoch in range(50, 75):\n",
    "    \n",
    "    # Train + Validation\n",
    "    training_loss = train_model(train_loader, model, epoch, writer)\n",
    "    val_loss = evaluate_model(val_loader, model, epoch, writer)\n",
    "    \n",
    "    # Val Stats\n",
    "    val_mAP_1, val_auc_1, _, val_mAP_2, val_auc_2, _ = evaluate_model_stats(eval_loader, model)\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print log of accuracy and loss\n",
    "    print(\"Epoch: \"+str(epoch)+\", Training loss: \"+str(training_loss)+\", Validation loss: \"+str(val_loss)+\", Validation mAP_1/AUC_1: \"+str(val_mAP_1)+\"/\"+str(val_auc_1)+\n",
    "             \", Validation mAP_2/AUC_2: \"+str(val_mAP_2)+\"/\"+str(val_auc_2))\n",
    "    \n",
    "    writer.add_scalar(\"mAP_1/val\", val_mAP_1, epoch)\n",
    "    writer.add_scalar(\"AUC_1/val\", val_auc_1, epoch)\n",
    "    writer.add_scalar(\"mAP_2/val\", val_mAP_2, epoch)\n",
    "    writer.add_scalar(\"AUC_2/val\", val_auc_2, epoch)\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    model_filename = model_dir + 'epoch' + str(epoch) + '.pt'\n",
    "    if(epoch == 0):\n",
    "        torch.save(model, model_filename)\n",
    "    else:\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss': training_loss,}, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd94a4d3-ed08-462a-941c-3bdc1207cc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06311606 0.2349789  0.28584845 0.21351467 0.07582021 0.11721418\n",
      " 0.24063135 0.3187921  0.26047259 0.00846959 0.07972048 0.03822025\n",
      " 0.03900044 0.46335556 0.29915556 0.02327851 0.54584518 0.21613161\n",
      " 0.11977328 0.05249026 0.20177751 0.25433079 0.01739497 0.01436637\n",
      " 0.00549193 0.11225592 0.03039365 0.05764055 0.00943022 0.01850149\n",
      " 0.00383499 0.02142827 0.0034585  0.01113409 0.0029845  0.01040271\n",
      " 0.10495693]\n",
      "[0.4849965  0.7548158  0.80625304 0.87640509 0.75836296 0.7476617\n",
      " 0.74867629 0.90321502 0.79816792 0.4686218  0.69421203 0.68850315\n",
      " 0.73699597 0.86870828 0.88612716 0.70617116 0.71748438 0.79166325\n",
      " 0.7385268  0.66744381 0.76024377 0.8075298  0.42104    0.21902503\n",
      " 0.34636247 0.35514999 0.47696628 0.50763718 0.36769679 0.30530701\n",
      " 0.59708975 0.59931251 0.61342942 0.36548198 0.51235383 0.18438517\n",
      " 0.5743179 ]\n",
      "[0.31704265 0.10949412 0.39397953 0.65275689 0.12372001 0.49159679\n",
      " 0.16241909]\n",
      "[0.72181951 0.53032441 0.59207832 0.81420354 0.7501865  0.66301432\n",
      " 0.57262068]\n"
     ]
    }
   ],
   "source": [
    "# Test Data\n",
    "data_dir = './data/'\n",
    "\n",
    "sounds_data = np.load(data_dir + 'audioset_test_data.npy', allow_pickle=True)\n",
    "class1_index = np.load(data_dir + 'audioset_test_labels_1.npy', allow_pickle=True)\n",
    "class2_index = np.load(data_dir + 'audioset_test_labels_2.npy', allow_pickle=True)\n",
    "\n",
    "eval_data = AudioSet_Siamese_Eval(sounds_data, class1_index, class2_index, 42, 7)\n",
    "eval_args = dict(shuffle = False, batch_size = 256, num_workers=8, pin_memory=True)\n",
    "eval_loader = DataLoader(eval_data, **eval_args)\n",
    "\n",
    "# Load model\n",
    "model_save_dir = './models/siamese_strong_doubleweighted_lambda1_1.75_lambda2_1_lambda3_0.25/'\n",
    "model_num = 'epoch0.pt'\n",
    "model = torch.load(model_save_dir + model_num) \n",
    "model.to(device)\n",
    "\n",
    "# Load saved weights\n",
    "weights_dir = 'epoch15.pt'\n",
    "state = torch.load(model_save_dir + weights_dir)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "test_mAP_1, test_AUC_1, outputs_1, test_mAP_2, test_AUC_2, outputs_2 = evaluate_model_stats(eval_loader, model, reduction='none')\n",
    "\n",
    "print(test_mAP_1)\n",
    "print(test_AUC_1)\n",
    "print(test_mAP_2)\n",
    "print(test_AUC_2)\n",
    "\n",
    "# Save results to txt file\n",
    "result_dir = './results/'\n",
    "np.savetxt(result_dir + 'test_mAP_1.txt', test_mAP_1)\n",
    "np.savetxt(result_dir + 'test_AUC_1.txt', test_AUC_1)\n",
    "np.savetxt(result_dir + 'test_mAP_2.txt', test_mAP_2)\n",
    "np.savetxt(result_dir + 'test_AUC_2.txt', test_AUC_2)\n",
    "scipy.io.savemat(result_dir + 'siamese_doubleweighted_lambda1_2_lambda2_1_lambda3_0.25.mat', {'test_mAP_1': test_mAP_1, 'test_mAP_2': test_mAP_2, 'test_AUC_1': test_AUC_1, 'test_AUC_2': test_AUC_2})\n",
    "\n",
    "import csv\n",
    "with open('./results/predictions_1.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Class header\n",
    "    writer.writerow(np.arange(outputs_1.shape[1]))\n",
    "    \n",
    "    for i in range(outputs_1.shape[0]):\n",
    "        writer.writerow(outputs_1[i])\n",
    "        \n",
    "        \n",
    "with open('./results/predictions_2.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Class header\n",
    "    writer.writerow(np.arange(outputs_2.shape[1]))\n",
    "    \n",
    "    for i in range(outputs_2.shape[0]):\n",
    "        writer.writerow(outputs_2[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09743d51-f3ce-4855-8811-dc18b5d02efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203710, 128)\n",
      "(20371, 42)\n",
      "(20371, 7)\n",
      "[array([5]) array([ 1, 19]) array([16]) ... array([37]) array([25])\n",
      " array([ 1, 26, 36])]\n",
      "0.1236652059062863\n",
      "0.32157272611331\n",
      "0.6177389457996932\n",
      "0.6634638981250486\n"
     ]
    }
   ],
   "source": [
    "print(eval_data.data.shape)\n",
    "print(eval_data.logits_1.shape)\n",
    "print(eval_data.logits_2.shape)\n",
    "print(eval_data.labels1)\n",
    "\n",
    "print(np.mean(test_mAP_1))\n",
    "print(np.mean(test_mAP_2))\n",
    "print(np.mean(test_AUC_1))\n",
    "print(np.mean(test_AUC_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ceb21-efa7-4aaa-a03c-bd6408675459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./results/targets_1.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(np.arange(42))\n",
    "    \n",
    "    \n",
    "    for i in range(len(eval_data.labels1)):\n",
    "        targets_1 = np.zeros((42, ))\n",
    "        targets_1[eval_data.labels1[i]] = 1\n",
    "        writer.writerow(targets_1)    \n",
    "    \n",
    "with open('./results/targets_2.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(np.arange(7))\n",
    "    \n",
    "    \n",
    "    for i in range(len(eval_data.labels2)):\n",
    "        targets_2 = np.zeros((7, ))\n",
    "        targets_2[eval_data.labels2[i]] = 1\n",
    "        writer.writerow(targets_2)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09416a62-e934-41d1-bb03-e971189ad674",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(eval_data.data))\n",
    "print(outputs_1.shape)\n",
    "print(len(eval_data.labels1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0f6e1-e2be-48d0-8643-be539f8596cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class9_idx = np.where(train_data.logits_1[:, 9] == 1)[0]\n",
    "print(train_data.labels1[class9_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381779d-dfb3-4f0f-93bd-0dfd409f7df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e08be74e4bb0362e5bda3d4d7ef4a226802309dd7e83da34a1d18a7b627e5934"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
